import sys
import logging
from typing import List, Any, Optional, Dict
import re
import time
import json
import asyncio
import argparse
import uuid

from ugvp_protocol import UGVPProtocol
from utils import get_query_embedding, fetch_and_validate_content
from config import log

# Dependencies for StrandAgent
from strands import Agent as StrandsAgent
from strands.tools.mcp import MCPClient
from strands.models.litellm import LiteLLMModel
from strands.session.file_session_manager import FileSessionManager
import litellm
from config import log, Config 
import google.generativeai as genai  
from mongodb_client import MongoDBClient
 
# Configure logging for StrandsAgent (removed basicConfig to avoid overriding global config)
# The root logger is configured in config.py.
logging.getLogger("strands.multiagent").setLevel(logging.DEBUG) # Increased to DEBUG
logging.getLogger("strands.event_loop").setLevel(logging.DEBUG) # Added for more verbose streaming logs
logging.getLogger("strands.telemetry.metrics").setLevel(logging.WARNING)
logging.getLogger("litellm").setLevel(logging.WARNING)

# Helper function from ugvp_protocol/utils.py (assuming it's needed for StrandAgent)
def get_innermost_exception(e: Exception) -> Exception:
    """Recursively gets the innermost exception from a chain."""
    if hasattr(e, '__cause__') and e.__cause__ is not None:
        return get_innermost_exception(e.__cause__)
    if hasattr(e, 'args') and e.args and isinstance(e.args[0], Exception):
        return get_innermost_exception(e.args[0])
    return e

# Set agent-specific logging level (removed to be controlled by config.py)
# log.setLevel(logging.INFO) # This line is removed
db = MongoDBClient()
class StrandAgent:
    """
    A wrapper agent for the Strands framework, allowing interaction with Strands agents
    using a LiteLLM model and provided MCP tools.
    """
    def __init__(self, name: str, model_id: str = "gemini/gemini-2.5-flash", 
                 system_prompt: str = "", tools: Optional[List[Any]] = None):
        """
        Initializes the StrandAgent with a name, LiteLLM model, system prompt, and optional tools.
        @param name: Name of the agent.
        @param model_id: The LiteLLM model to use for generation.
        @param system_prompt: The initial system prompt for the Strands agent.
        @param tools: A list of MCP tools to be used by the agent.
        """
        self.name = name
        self.session_manager = FileSessionManager(session_id=str(uuid.uuid4()))
        self.model = LiteLLMModel(
            model_id=model_id,
            params={
                "max_token": 5000,
                "temperature": 0
            }
        )
        self.base_system_prompt = system_prompt
        self.tools = tools if tools is not None else []
        log.info(f"StrandAgent '{self.name}' initialized with LiteLLMModel '{model_id}'.")

    async def run(self, user_input: str, override_system_prompt: Optional[str] = None) -> str:
        """
        Runs a single interaction with the underlying Strands agent.
        @param user_input: The input string from the user.
        @param override_system_prompt: An optional system prompt to use for this specific run,
                                       overriding the one provided during initialization.
        @return: The full response generated by the Strands agent.
        """
        current_system_prompt = override_system_prompt if override_system_prompt else self.base_system_prompt
        
        if self.tools:
            tool_strings = []
            for spec in self.tools:
                name = spec.tool_spec.get('name', 'Unnamed Tool')
                description = spec.tool_spec.get('description', 'No description')
                tool_strings.append(f"- {name}: {description}")
            current_system_prompt += "\n\nHere are the available tools:\n" + "\n".join(tool_strings)
        
        agent = StrandsAgent(
            model=self.model,
            tools=self.tools,
            system_prompt=current_system_prompt,
            session_manager=self.session_manager
        )
        if not user_input.strip():
            log.warning("No input provided to the Strand Agent.")
            return "No input provided to the Strand Agent."
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response_chunks = [] # Use a list to accumulate chunks
                is_first_chunk = True
                agent_stream = agent.stream_async(user_input)
                async for event in agent_stream :
                    if event.get("type") == "contentBlockDelta":
                        if is_first_chunk:
                            sys.stdout.write("Agent: ")
                            sys.stdout.flush()
                            is_first_chunk = False
                        delta = event.get("delta", {})
                        if "text" in delta:
                            text_chunk = delta["text"]
                            response_chunks.append(text_chunk) # Append to list
                            sys.stdout.write(text_chunk)
                            sys.stdout.flush()
                            log.debug(f"Agent stream chunk: {text_chunk}") # Log each chunk to file
                            # log.debug(f"Accumulated response_chunks (in loop): {''.join(response_chunks)[:500]}...") # Debug accumulated response
                    elif event.get("type") == "toolUse":
                        tool_name = event.get("name")
                        tool_input = event.get("input")
                        log.info(f"Agent using tool: {tool_name} with input: {tool_input}")
                        is_first_chunk = True
                    elif event.get("type") == "toolOutput":
                        content = event.get("content", [])
                        log.info(f"Agent tool output: {content}")
                        pass # Suppress tool output for non-interactive run
                    # Removed break on messageStop to ensure full response accumulation
                full_response = "".join(response_chunks) # Join chunks at the end
                log.debug(f"Accumulated response_chunks (final): {response_chunks}") # Log the full list of chunks
                log.debug(f"Final full_response before return (in StrandAgent.run): {full_response[:500]}...") # Debug final full_response
                log.info(f"StrandAgent returning full_response: {full_response[:500]}...") # Log final response
                return full_response
            except AttributeError as e:
                if "'ModelResponseStream' object has no attribute 'usage'" in str(e):
                    log.warning(f"Caught expected AttributeError in Strands: {e}. This is likely due to a LiteLLM/Strands version incompatibility. Continuing without usage info.")
                    full_response = "".join(response_chunks) # Ensure full response is captured
                    log.debug(f"Accumulated response_chunks (final, after AttributeError): {response_chunks}")
                    log.debug(f"Final full_response (after AttributeError): {full_response[:500]}...")
                    return full_response
                else:
                    log.error(f"An unexpected AttributeError occurred: {e}", exc_info=True)
                    break
            except Exception as e:
                log.debug(f"Caught exception type: {type(e)}")
                innermost_exception = get_innermost_exception(e.__cause__ if e.__cause__ else e) # More robust innermost exception
                log.debug(f"Innermost exception type: {type(innermost_exception)}")
                if isinstance(innermost_exception, litellm.exceptions.RateLimitError):
                    retry_delay = 15
                    try:
                        match = re.search(r'"retryDelay": "(\d+)s"', str(innermost_exception))
                        if match:
                            retry_delay = int(match.group(1)) + 1
                    except Exception:
                        pass
                    log.warning(f"Rate limit hit: {innermost_exception}. Retrying in {retry_delay} seconds...")
                    time.sleep(retry_delay)
                elif isinstance(e, (litellm.ServiceUnavailableError, litellm.exceptions.MidStreamFallbackError)):
                    log.warning(f"Service unavailable error: {e}. Retrying in 5 seconds...")
                    time.sleep(5)
                else:
                    log.error(f"An unexpected error occurred: {e}", exc_info=True)
                    break
        log.error("Strand Agent failed to generate a response after multiple retries.")
        return "Strand Agent failed to generate a response after multiple retries."


class GroundingAgent:
    """
    The 'Never Lies' PoC Agent: Generates content and self-verifies every claim
    against the actual published source using the UGVP.
    """
    
    def __init__(self, ugvp: UGVPProtocol, strategy: str = 'agent'):
        """
        Initializes the GroundingAgent.
        @param ugvp: An instance of the UGVPProtocol.
        @param strategy: The generation strategy ('agent' or 'llm').
                         'agent': The agent constructs the IGM and passes it to the LLM (default).
                         'llm': The LLM is prompted to generate the IGM and SSR according to the protocol.
        """
        self.ugvp = ugvp
        self.strategy = strategy
        self.ssr_cache: Dict[str, dict] = {} # Caches verified SSR entries for final assembly
        if self.strategy not in ['agent', 'llm']:
            raise ValueError("Strategy must be either 'agent' or 'llm'.")
        log.info(f"GroundingAgent initialized with strategy: '{self.strategy}'")

    def _pre_response_hook(self):
        """
        A production hook for auditing and logging before the final response is issued.
        """
        log.info("⚙️ HOOK: Final integrity review complete. Response ready for output.")

    def _prepare_claim_generation_prompt(self, query: str, context: Dict[str, Any]) -> str:
        """Prepares a prompt that asks the LLM to generate the claim text."""
        content_text = context.get('text_chunk') or context.get('content')
        if not content_text:
            raise ValueError("Context is missing 'text_chunk' or 'content'.")

        return (
            f"Based on the following context, generate a comprehensive answer that addresses the user's query: '{query}'. "
            f"Ensure the answer is factual and directly supported by the context. Do not add any extra formatting, markers, or conversational text.\n\n"
            f"Context: {content_text}"
        )

    def _prepare_llm_strategy_prompt(self, query: str, context: Dict[str, Any]) -> str:
        """Prepares the prompt for the 'llm' strategy, instructing the LLM to follow the UGVP."""
        content_text = context.get('text_chunk') or context.get('content')
        if not content_text:
            raise ValueError("Context is missing 'text_chunk' or 'content'.")

        full_shi = context.get('shi')
        if not full_shi:
            raise ValueError(f"Could not find SHI in the provided context for doc_id: {context.get('doc_id')}")
        shi_prefix = full_shi[:Config.SHI_PREFIX_LENGTH] # Use the defined prefix length for the prompt

        location_info = context.get('loc_selector')
        if not location_info:
            location_info = f"para:{context.get('chunk_id', 'N/A')}"

        return (
            f"You are an AI assistant that strictly follows the Audited Context Generation protocol (ACG).\n"
            f"Your task is to answer the user's query based *only* on the provided context. "
            f"After making a factual claim, you MUST embed an Inline Grounding Marker (IGM) in the format `[C{{N}}:{shi_prefix}:{location_info}]`.\n"
            f"If you synthesize information from multiple claims, you MUST also embed a Relationship Marker (RM) in the format `(R{{M}}:TYPE:C{{1}},C{{2}},...)`.\n"
            f"After your answer, you MUST append an ACG block containing both Structured Source Registry (SSR) and Veracity Audit Registry (VAR) entries.\n"
            f"The ACG block must start with '--- ACG_START ---' and end with '--- ACG_END ---'.\n"
            f"Inside this block, provide a JSON object with two keys: 'SSR' (a list of SSR entries) and 'VAR' (a list of VAR entries).\n"
            f"Each SSR entry should contain: 'SHI', 'Type', 'Canonical_URI', 'Location_Type', 'Loc_Selector'.\n"
            f"Each VAR entry should contain: 'RELATION_ID', 'TYPE', 'DEP_CLAIMS' (list of CIDs), 'SYNTHESIS_PROSE', 'LOGIC_MODEL', 'AUDIT_STATUS' (initially 'PENDING'), 'TIMESTAMP'.\n"
            f"User Query: '{query}'\n\n"
            f"Context from source document:\n---\n{content_text}\n---\n\n"
            f"Generate your response now. First, provide the sentence answering the query by *directly quoting* the relevant sentence from the context, followed by the IGM and optionally the RM. Then, append the ACG block with the generated SSR and VAR entries. Do not add any other text."
        )

    async def generate_and_verify(self, query: str) -> str:
        """
        End-to-end process: RAG (Vector Search) -> Generation -> Verification -> Output.
        """
        # 1. Retrieval (RAG)
        query_embedding = get_query_embedding(query)
        search_results = db.vector_search(query_embedding, limit=2, min_score=0.85)
        log.info(f"🔍 [RAG] Vector search returned {len(search_results)} candidate(s).")
        if not search_results:
            return "❌ Agent unable to retrieve sftrong, unique grounding context. Halting to prevent hallucination."

        verified_claims_texts = []
        final_verified_ssr: Dict[str, dict] = {}
        final_verified_var: Dict[str, dict] = {}

        for i, context in enumerate(search_results):
            log.info(f"--- Processing candidate {i+1}/{len(search_results)} ---")
            
            log.debug(f"\n--- New Interaction (Candidate {i+1}) ---\n")
            log.debug(f"Timestamp: {time.ctime()}\n")
            log.debug(f"Query: {query}\n")
            log.debug(f"Injected Context: {json.dumps(context, indent=2)}\n")

            # 2. Generation (Using StrandAgent)
            if self.strategy == 'agent':
                strand_prompt = self._prepare_claim_generation_prompt(query, context)
                system_prompt = "You are a helpful assistant that provides comprehensive answers based on the provided context."
            elif self.strategy == 'llm':
                try:
                    strand_prompt = self._prepare_llm_strategy_prompt(query, context)
                except ValueError as e:
                    log.warning(f"Skipping candidate {i+1} due to prompt preparation error: {e}")
                    continue
                system_prompt = "You are an AI assistant that strictly follows the Audited Context Generation protocol (ACG)."
            else:
                raise RuntimeError("Invalid strategy configured.")

            log.debug(f"Strand Agent Prompt: {strand_prompt}\n")

            strand_agent = StrandAgent(name=f"ACG_Generator_{i+1}", system_prompt=system_prompt)
            generated_text = await strand_agent.run(strand_prompt)

            full_generated_output = ""
            text_only_output = ""

            full_generated_output = ""
            text_only_output = ""

            if self.strategy == 'agent':
                # For 'agent' strategy, we explicitly create a relationship_metadata for the generated claim
                # This ensures a VAR entry is generated even if the LLM doesn't explicitly create RMs.
                relationship_metadata_for_agent = {
                    "RELATION_ID": f"R{i+1}", # Unique ID for each candidate's claim
                    "TYPE": "SUMMARY", # Assuming a summary for a single claim
                    "DEP_CLAIMS": ["C1"], # The single claim generated by the agent
                    "SYNTHESIS_PROSE": f"Claim generated by agent for candidate {i+1}",
                    "LOGIC_MODEL": "Agent_Claim_Generation_Model_v1.0",
                    "AUDIT_STATUS": "PENDING",
                    "TIMESTAMP": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
                }
                try:
                    full_generated_output, ssr_entry, var_entry = self.ugvp.generate_grounded_text(
                        generated_text.strip(), context, relationship_metadata=relationship_metadata_for_agent
                    )
                    log.debug(f"full_generated_output :: {full_generated_output}")
                    text_only_output = re.sub(r"--- ACG_START ---.*?--- ACG_END ---", "", full_generated_output, flags=re.DOTALL).strip()
                    if ssr_entry:
                        final_verified_ssr[ssr_entry['SHI']] = {**ssr_entry, "Verification_Status": "PENDING"}
                    if var_entry:
                        final_verified_var[var_entry['RELATION_ID']] = {**var_entry, "AUDIT_STATUS": "PENDING"}
                except (ValueError, AttributeError):
                    log.warning(f"Could not construct grounded text for candidate {i+1}. Raw LLM output: '{generated_text}'")
                    text_only_output = generated_text
                    full_generated_output = generated_text
            else: # self.strategy == 'llm'
                full_generated_output = generated_text
                text_only_output = re.sub(r"--- ACG_START ---.*?--- ACG_END ---", "", full_generated_output, flags=re.DOTALL).strip()

            log.debug(f"GroundingAgent raw_output before parsing ACG data: {full_generated_output[:500]}...")
            
            # 3. Verification Phase 1: Atomic Fact Verification (UGVP)
            igms, rms, ssr_from_llm, var_from_llm = self.ugvp.parse_acg_data(full_generated_output)
            verified_igms_count = 0

            log.info(f"✅ [Phase 1 Verify] Initiating checks for {len(igms)} claim(s) in candidate {i+1}...")

            for igm in igms:
                # Use find_shi_metadata to retrieve the full SSR entry based on the SHI prefix from the IGM
                full_metadata = db.find_shi_metadata(igm['shi'])
                
                if full_metadata:
                    is_valid, reason = fetch_and_validate_content(
                        uri=full_metadata['Canonical_URI'],
                        selector=f"css={igm['loc']}",
                        expected_snippet=igm['claim_context']
                    )
                    if is_valid:
                        final_ssr_entry = {**full_metadata, "Verification_Status": "VERIFIED"}
                        db.insert_ssr_entry(final_ssr_entry)
                        final_verified_ssr[final_ssr_entry['SHI']] = final_ssr_entry
                        verified_igms_count += 1
                    else:
                        log.warning(f"❌ Claim {igm['claim_id']} from candidate {i+1} failed structural validation. Reason: {reason}. IGNORED.")
                        # Mark corresponding SSR entry as FAILED
                        final_verified_ssr[full_metadata['SHI']] = {**full_metadata, "Verification_Status": "FAILED"}
                else:
                    log.error(f"❌ Claim {igm['claim_id']} from candidate {i+1} failed lookup (SHI prefix: {igm['shi']} not found in DB).")
            
            # Phase 2: Synthesis Verification (RSVP)
            log.info(f"✅ [Phase 2 Verify] Initiating checks for {len(rms)} relationship(s) in candidate {i+1}...")
            for rm in rms:
                var_entry = var_from_llm.get(rm['relationship_id'])
                if var_entry:
                    all_deps_verified = True
                    for dep_claim_id in rm['dep_claims']:
                        # Find the SHI for the dependency claim
                        dep_shi = next((igm['shi'] for igm in igms if igm['claim_id'] == dep_claim_id), None)
                        if not dep_shi or final_verified_ssr.get(dep_shi, {}).get("Verification_Status") != "VERIFIED":
                            all_deps_verified = False
                            break
                    
                    if all_deps_verified:
                        logic_model_name = var_entry.get('LOGIC_MODEL', 'Default_Verification_Model_v1.0')
                        # Placeholder for actual logic model invocation
                        # For now, we assume VERIFIED_LOGIC if all dependencies are verified.
                        var_entry["AUDIT_STATUS"] = "VERIFIED_LOGIC" 
                        log.info(f"Logic Model '{logic_model_name}' invoked for RM {rm['relationship_id']}. Status: {var_entry['AUDIT_STATUS']}")
                    else:
                        var_entry["AUDIT_STATUS"] = "INSUFFICIENT_PREMISE"
                    
                    final_verified_var[var_entry['RELATION_ID']] = var_entry
                else:
                    log.error(f"❌ Relationship {rm['relationship_id']} from candidate {i+1} failed lookup (RM not found in LLM's VAR).")

            if verified_igms_count > 0:
                verified_claims_texts.append(text_only_output)
            else:
                log.warning(f"No claims verified for candidate {i+1}. Discarding its output.")
            
            log.debug(f"--- End Interaction (Candidate {i+1}) ---\n")

        # 4. Final Synthesis from Verified Claims
        if not verified_claims_texts:
            log.error("Agent generated content, but no claims could be successfully verified against the sources.")
            return "❌ Agent generated content, but no claims could be successfully verified against the sources."

        # Combine all verified claims into a single context for final synthesis
        combined_verified_context = "\n\n".join(verified_claims_texts)

        # Create a new prompt for the final synthesis agent
        # Temporarily simplify the synthesis prompt for debugging
        synthesis_prompt = (
            f"Synthesize an answer to the query: '{query}' based on the following verified information:\n\n"
            f"Verified Information:\n---\n{combined_verified_context}\n---\n\n"
            f"Provide the synthesized answer now."
        )

        # Use google.generativeai directly for synthesis, bypassing StrandAgent
        log.info("Attempting final synthesis directly with google.generativeai...")
        try:
            genai.configure(api_key=Config.GOOGLE_API_KEY)
            # Use a generative model for synthesis, not an embedding model
            synthesis_model = genai.GenerativeModel("gemini-2.5-flash") 
            
            # Generate content
            response = await synthesis_model.generate_content_async(synthesis_prompt) # Use async version
            final_synthesized_answer = response.text
            
            log.debug(f"Final synthesized answer before ACG assembly: {final_synthesized_answer}")
            print(f"\n--- DEBUG: Final synthesized answer content ---\n{final_synthesized_answer}\n--- END DEBUG ---\n")
            
        except Exception as e:
            log.error(f"Error during direct synthesis with google.generativeai: {e}", exc_info=True)
            final_synthesized_answer = "Error: Could not generate final synthesized answer."
        # For the final synthesized answer, create a top-level SUMMARY relationship
        # Collect all claim_ids from the verified SSR entries that contributed to the synthesis
        all_verified_claim_ids = [igm['claim_id'] for igm_list, _, _, _ in [self.ugvp.parse_acg_data(text) for text in verified_claims_texts] for igm in igm_list]

        # Dynamically determine the relationship type for final synthesis
        # This is a placeholder for a more sophisticated logic model that would analyze the synthesis
        # For now, if there's more than one claim, it's a SUMMARY. If one, it's a direct INFERENCE.
        final_synthesis_rel_type = "SUMMARY" if len(all_verified_claim_ids) > 1 else "INFERENCE"

        final_synthesis_relationship_metadata = {
            "RELATION_ID": f"R_FINAL_SYNTHESIS_{uuid.uuid4().hex[:8]}", # Unique ID for final synthesis
            "TYPE": final_synthesis_rel_type,
            "DEP_CLAIMS": list(set(all_verified_claim_ids)), # Ensure unique claim IDs
            "SYNTHESIS_PROSE": f"Final synthesized answer to query: '{query}'",
            "LOGIC_MODEL": "ACG_Final_Synthesizer_Model_v1.0", # Specific model for final synthesis
            "AUDIT_STATUS": "PENDING",
            "TIMESTAMP": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        }

        # Add this final synthesis VAR entry to the overall final_verified_var
        final_verified_var[final_synthesis_relationship_metadata['RELATION_ID']] = {
            **final_synthesis_relationship_metadata,
            "AUDIT_STATUS": "VERIFIED_LOGIC" # Assuming the synthesis logic is sound for PoC
        }

        # 5. Final Compilation and Hook
        self._pre_response_hook()

        # The final_synthesized_answer itself doesn't contain ACG markers,
        # so we pass it directly and the assemble_final_output will add the ACG block.
        final_document = self.ugvp.assemble_final_output(final_synthesized_answer, final_verified_ssr, final_verified_var)

        log.info(f"Final GroundingAgent response: {final_document}") # Log the final document
        return final_document

# --- Terminal Invocation ---
if __name__ == "__main__":
    
    async def main(): # Define an async main function
        parser = argparse.ArgumentParser(description="Run the UGVP Grounding Agent.")
        parser.add_argument(
            '--strategy', 
            type=str, 
            default='agent', 
            choices=['agent', 'llm'],
            help="The generation strategy to use: 'agent' (default) or 'llm'."
        )
        args = parser.parse_args()

        try:
            # 1. Initialize Core Components
            ugvp_instance = UGVPProtocol()
            agent = GroundingAgent(ugvp_instance, strategy=args.strategy)

            log.info(f"\n--- ACG Grounding Agent (PoC) Activated [Strategy: {args.strategy}] ---")
            log.info("This agent connects to MongoDB and verifies claims against live web pages.")
            
            rag_doc_count = db.count_rag_documents()
            if rag_doc_count == 0:
                log.warning("⚠️ RAG collection is empty. Please run the indexer to populate the database.")
                log.warning("Example: python ugvp_protocol/indexer.py <URL_TO_INDEX>")
            else:
                log.info(f"✅ RAG collection contains {rag_doc_count} documents.")

            log.info("Enter your query (e.g., 'What is the latest climate data?'). Type 'exit' to quit.")
            
            while True:
                user_input = input("\n[USER]: ")
                log.info(f"[USER]: {user_input}") # Log user input
                
                if user_input.lower() in ['exit', 'q']:
                    log.info("Exiting agent. Goodbye.")
                    break
                    
                if not user_input.strip():
                    continue
                
                try:
                    response = await agent.generate_and_verify(user_input) # Await the async method
                    print("\n[AGENT RESPONSE]:") # Keep console output for final response
                    print(response)
                    log.info(f"[AGENT RESPONSE]: {response}")
                except Exception as e:
                    log.error(f"An error occurred during generation/verification: {e}", exc_info=True)
                    print("\n[AGENT ERROR]: Sorry, an internal error occurred. Please try another query.")
        except Exception as e:
            log.critical(f"Agent failed to start/run due to critical error: {e}", exc_info=True)
            sys.exit(1)

    asyncio.run(main()) # Run the async main function
