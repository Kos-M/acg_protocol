import sys
import logging
from typing import List, Any, Optional, Dict
import re
import time
import json
import asyncio
import argparse
import uuid

from ugvp_protocol import UGVPProtocol
from utils import get_query_embedding, fetch_and_validate_content
from config import log

from strands import Agent as StrandsAgent
from strands.models.litellm import LiteLLMModel
from strands.session.file_session_manager import FileSessionManager
import litellm
from config import log, Config 
import google.generativeai as genai  
from mongodb_client import MongoDBClient
 
logging.getLogger("strands.multiagent").setLevel(logging.ERROR)
logging.getLogger("strands.event_loop").setLevel(logging.ERROR)
logging.getLogger("strands.telemetry.metrics").setLevel(logging.ERROR)
logging.getLogger("litellm").setLevel(logging.ERROR)

def get_innermost_exception(e: Exception) -> Exception:
    """Recursively gets the innermost exception from a chain."""
    if hasattr(e, '__cause__') and e.__cause__ is not None:
        return get_innermost_exception(e.__cause__)
    if hasattr(e, 'args') and e.args and isinstance(e.args[0], Exception):
        return get_innermost_exception(e.args[0])
    return e

db = MongoDBClient()
class StrandAgent:
    """
    A wrapper agent for the Strands framework, allowing interaction with Strands agents
    using a LiteLLM model and provided MCP tools.
    """
    def __init__(self, name: str, model_id: str = "gemini/gemini-2.5-flash", 
                 system_prompt: str = "", tools: Optional[List[Any]] = None):
        """
        Initializes the StrandAgent with a name, LiteLLM model, system prompt, and optional tools.
        @param name: Name of the agent.
        @param model_id: The LiteLLM model to use for generation.
        @param system_prompt: The initial system prompt for the Strands agent.
        @param tools: A list of MCP tools to be used by the agent.
        """
        self.name = name
        self.session_manager = FileSessionManager(session_id=str(uuid.uuid4()))
        self.model = LiteLLMModel(
            model_id=model_id,
            params={
                "max_token": 5000,
                "temperature": 0
            }
        )
        self.base_system_prompt = system_prompt
        self.tools = tools if tools is not None else []

    async def close(self):
        """
        Closes any underlying resources, such as LiteLLM client sessions.
        """
        if hasattr(self.model, 'aclose') and callable(self.model.aclose):
            await self.model.aclose()
        elif hasattr(self.model, 'client') and hasattr(self.model.client, 'aclose'):
            await self.model.client.aclose()
        elif hasattr(litellm, 'client') and hasattr(litellm.client, 'aclose'):
            await litellm.client.aclose()
        else:
            pass

    async def run(self, user_input: str, override_system_prompt: Optional[str] = None) -> str:
        """
        Runs a single interaction with the underlying Strands agent.
        @param user_input: The input string from the user.
        @param override_system_prompt: An optional system prompt to use for this specific run,
                                       overriding the one provided during initialization.
        @return: The full response generated by the Strands agent.
        """
        current_system_prompt = override_system_prompt if override_system_prompt else self.base_system_prompt
        
        if self.tools:
            tool_strings = []
            for spec in self.tools:
                name = spec.tool_spec.get('name', 'Unnamed Tool')
                description = spec.tool_spec.get('description', 'No description')
                tool_strings.append(f"- {name}: {description}")
            current_system_prompt += "\n\nHere are the available tools:\n" + "\n".join(tool_strings)
        
        agent = StrandsAgent(
            model=self.model,
            tools=self.tools,
            system_prompt=current_system_prompt,
            # session_manager=self.session_manager
        )
        if not user_input.strip():
            return "No input provided to the Strand Agent."
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # Disable streaming by collecting all chunks before returning
                response_chunks = []
                agent_stream = agent.stream_async(user_input)
                async for event in agent_stream:
                    if event.get("type") == "contentBlockDelta":
                        delta = event.get("delta", {})
                        if "text" in delta:
                            text_chunk = delta["text"]
                            response_chunks.append(text_chunk)
                    elif event.get("type") == "toolUse":
                        pass
                    elif event.get("type") == "toolOutput":
                        pass
                
                full_response = "".join(response_chunks)
                return full_response
            except AttributeError as e:
                if "'ModelResponseStream' object has no attribute 'usage'" in str(e):
                    full_response = "".join(response_chunks)
                    return full_response
                else:
                    log.error(f"An unexpected AttributeError occurred: {e}", exc_info=True)
                    break
            except Exception as e:
                innermost_exception = get_innermost_exception(e.__cause__ if e.__cause__ else e)
                if isinstance(innermost_exception, litellm.exceptions.RateLimitError):
                    retry_delay = 15
                    try:
                        match = re.search(r'"retryDelay": "(\d+)s"', str(innermost_exception))
                        if match:
                            retry_delay = int(match.group(1)) + 1
                    except Exception:
                        pass
                    log.warning(f"Rate limit hit. Retrying in {retry_delay} seconds...")
                    time.sleep(retry_delay)
                elif isinstance(e, (litellm.ServiceUnavailableError, litellm.exceptions.MidStreamFallbackError)):
                    log.warning(f"Service unavailable error. Retrying in 5 seconds...")
                    time.sleep(5)
                else:
                    log.error(f"An unexpected error occurred: {e}", exc_info=True)
                    break
        log.error("Strand Agent failed to generate a response after multiple retries.")
        return "Strand Agent failed to generate a response after multiple retries."


class GroundingAgent:
    """
    The 'Never Lies' PoC Agent: Generates content and self-verifies every claim
    against the actual published source using the UGVP.
    """
    
    def __init__(self, ugvp: UGVPProtocol, strategy: str = 'agent'):
        """
        Initializes the GroundingAgent.
        @param ugvp: An instance of the UGVPProtocol.
        @param strategy: The generation strategy ('agent' or 'llm').
                         'agent': The agent constructs the IGM and passes it to the LLM (default).
                         'llm': The LLM is prompted to generate the IGM and SSR according to the protocol.
        """
        self.ugvp = ugvp
        self.strategy = strategy
        self.ssr_cache: Dict[str, dict] = {}
        if self.strategy not in ['agent', 'llm']:
            raise ValueError("Strategy must be either 'agent' or 'llm'.")

    def _pre_response_hook(self):
        pass

    def _prepare_claim_generation_prompt(self, query: str, context: Dict[str, Any]) -> str:
        """Prepares a prompt that asks the LLM to generate the claim text."""
        content_text = context.get('text_chunk') or context.get('content')
        if not content_text:
            raise ValueError("Context is missing 'text_chunk' or 'content'.")

        return (
            f"Based on the following context, generate a comprehensive answer that addresses the user's query: '{query}'. "
            f"Ensure the answer is factual and directly supported by the context. Do not add any extra formatting, markers, or conversational text.\n\n"
            f"Context: {content_text}"
        )

    def _prepare_llm_strategy_prompt(self, query: str, context: Dict[str, Any]) -> str:
        """Prepares the prompt for the 'llm' strategy, instructing the LLM to follow the UGVP."""
        content_text = context.get('text_chunk') or context.get('content')
        if not content_text:
            raise ValueError("Context is missing 'text_chunk' or 'content'.")

        full_shi = context.get('shi')
        if not full_shi:
            raise ValueError(f"Could not find SHI in the provided context for doc_id: {context.get('doc_id')}")
        shi_prefix = full_shi[:Config.SHI_PREFIX_LENGTH] # Use the defined prefix length for the prompt

        location_info = context.get('loc_selector')
        if not location_info:
            location_info = f"para:{context.get('chunk_id', 'N/A')}"

        return (
            f"You are an AI assistant that strictly follows the Audited Context Generation protocol (ACG).\n"
            f"Your task is to answer the user's query based *only* on the provided context. "
            f"After making a factual claim, you MUST embed an Inline Grounding Marker (IGM) in the format `[C{{N}}:{shi_prefix}:{location_info}]`.\n"
            f"If you synthesize information from multiple claims, you MUST also embed a Relationship Marker (RM) in the format `(R{{M}}:TYPE:C{{1}},C{{2}},...)`.\n"
            f"After your answer, you MUST append an ACG block containing both Structured Source Registry (SSR) and Veracity Audit Registry (VAR) entries.\n"
            f"The ACG block must start with '--- ACG_START ---' and end with '--- ACG_END ---'.\n"
            f"Inside this block, provide a JSON object with two keys: 'SSR' (a list of SSR entries) and 'VAR' (a list of VAR entries).\n"
            f"Each SSR entry should contain: 'SHI', 'Type', 'Canonical_URI', 'Location_Type', 'Loc_Selector'.\n"
            f"Each VAR entry should contain: 'RELATION_ID', 'TYPE', 'DEP_CLAIMS' (list of CIDs), 'SYNTHESIS_PROSE', 'LOGIC_MODEL', 'AUDIT_STATUS' (initially 'PENDING'), 'TIMESTAMP'.\n"
            f"User Query: '{query}'\n\n"
            f"Context from source document:\n---\n{content_text}\n---\n\n"
            f"Generate your response now. First, provide the sentence answering the query by *directly quoting* the relevant sentence from the context, followed by the IGM and optionally the RM. Then, append the ACG block with the generated SSR and VAR entries. Do not add any other text."
        )

    async def generate_and_verify(self, query: str) -> str:
        """
        End-to-end process: RAG (Vector Search) -> Generation -> Verification -> Output.
        """
        # 1. Retrieval (RAG)
        query_embedding = get_query_embedding(query)
        search_results = db.vector_search(query_embedding, limit=3, min_score=0.85)
        log.info(f"🔍 [RAG] Vector search returned {len(search_results)} candidate(s).")
        if not search_results:
            return "❌ Agent unable to retrieve strong, unique grounding context. Halting to prevent hallucination.", []

        verified_claims_texts = []
        final_verified_ssr: Dict[str, dict] = {}
        final_verified_var: Dict[str, dict] = {}

        for i, context in enumerate(search_results):
            log.info(f"--- Processing candidate {i+1}/{len(search_results)} ---")
            
            log.debug(f"\n--- New Interaction (Candidate {i+1}) ---\n")
            log.debug(f"Timestamp: {time.ctime()}\n")
            log.debug(f"Query: {query}\n")
            log.debug(f"Injected Context: {json.dumps(context, indent=2)}\n")

            # 2. Generation (Using StrandAgent)
            if self.strategy == 'agent':
                strand_prompt = self._prepare_claim_generation_prompt(query, context)
                system_prompt = "You are a helpful assistant that provides comprehensive answers based on the provided context."
            elif self.strategy == 'llm':
                try:
                    strand_prompt = self._prepare_llm_strategy_prompt(query, context)
                except ValueError as e:
                    log.warning(f"Skipping candidate {i+1} due to prompt preparation error: {e}")
                    continue
                system_prompt = "You are an AI assistant that strictly follows the Audited Context Generation protocol (ACG)."
            else:
                raise RuntimeError("Invalid strategy configured.")

            strand_agent = StrandAgent(name=f"ACG_Generator_{i+1}", system_prompt=system_prompt)
            try:
                generated_text = await strand_agent.run(strand_prompt)
            finally:
                await strand_agent.close() # Ensure the agent's resources are closed

            full_generated_output = ""
            text_only_output = ""

            if self.strategy == 'agent':
                # For 'agent' strategy, we explicitly create a relationship_metadata for the generated claim
                # This ensures a VAR entry is generated even if the LLM doesn't explicitly create RMs.
                relationship_metadata_for_agent = {
                    "RELATION_ID": f"R{i+1}",
                    "TYPE": "SUMMARY",
                    "DEP_CLAIMS": ["C1"],
                    "SYNTHESIS_PROSE": f"Claim generated by agent for candidate {i+1}",
                    "LOGIC_MODEL": "Agent_Claim_Generation_Model_v1.0",
                    "AUDIT_STATUS": "PENDING",
                    "TIMESTAMP": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
                }
                try:
                    full_generated_output, ssr_entry, var_entry = self.ugvp.generate_grounded_text(
                        generated_text.strip(), context, relationship_metadata=relationship_metadata_for_agent
                    )
                    log.debug(f"full_generated_output :: {full_generated_output}")
                    text_only_output = re.sub(r"--- ACG_START ---.*?--- ACG_END ---", "", full_generated_output, flags=re.DOTALL).strip()
                    if ssr_entry:
                        final_verified_ssr[ssr_entry['SHI']] = {**ssr_entry, "Verification_Status": "PENDING"}
                    if var_entry:
                        final_verified_var[var_entry['RELATION_ID']] = {**var_entry, "AUDIT_STATUS": "PENDING"}
                except (ValueError, AttributeError):
                    log.warning(f"Could not construct grounded text for candidate {i+1}. Raw LLM output: '{generated_text}'")
                    text_only_output = generated_text
                    full_generated_output = generated_text
            else:
                full_generated_output = generated_text
                text_only_output = re.sub(r"--- ACG_START ---.*?--- ACG_END ---", "", full_generated_output, flags=re.DOTALL).strip()
            
            igms, rms, ssr_from_llm, var_from_llm = self.ugvp.parse_acg_data(full_generated_output)
            verified_igms_count = 0

            log.info(f"✅ [Phase 1 Verify] Initiating checks for {len(igms)} claim(s) in candidate {i+1}...")

            for igm in igms:
                # Use the source_uri and full SHI from the current candidate's context directly for verification
                # The igm['loc'] already contains the CSS selector part.
                is_valid, reason = fetch_and_validate_content(
                    uri=context['source_uri'], # Use source_uri from the current context
                    selector=f"css={igm['loc']}",
                    expected_snippet=igm['claim_context']
                )
                
                # Create the SSR entry using information from the context and the igm
                # This entry will be inserted into the DB if valid, or tracked internally if failed.
                current_ssr_entry = {
                    "SHI": context['shi'], # Full SHI from context
                    "Type": "Web Article", # Assuming type for now, could be derived from context metadata
                    "Canonical_URI": context['source_uri'],
                    "Location_Type": "CSS_Selector",
                    "Loc_Selector": igm['loc'],
                    "Chunk_ID": context['chunk_id'], # Chunk_ID from context
                    "Verification_Status": "PENDING" # Initial status
                }

                # Use a composite key (SHI-Chunk_ID) for final_verified_ssr to avoid overwriting
                ssr_key = f"{current_ssr_entry['SHI']}-{current_ssr_entry['Chunk_ID']}"

                if is_valid:
                    current_ssr_entry["Verification_Status"] = "VERIFIED"
                    db.insert_ssr_entry(current_ssr_entry) # Insert into DB
                    final_verified_ssr[ssr_key] = current_ssr_entry
                    verified_igms_count += 1
                else:
                    current_ssr_entry["Verification_Status"] = "FAILED"
                    log.warning(f"❌ Claim {igm['claim_id']} from candidate {i+1} failed structural validation. Reason: {reason}. IGNORED.")
                    # Even if validation fails, we track the failed SSR entry internally for the final ACG block
                    final_verified_ssr[ssr_key] = current_ssr_entry
            
            # Phase 2: Synthesis Verification (RSVP)
            log.info(f"✅ [Phase 2 Verify] Initiating checks for {len(rms)} relationship(s) in candidate {i+1}...")
            for rm in rms:
                var_entry = var_from_llm.get(rm['relationship_id'])
                if var_entry:
                    all_deps_verified = True
                    for dep_claim_id in rm['dep_claims']:
                        # Need to find the full SHI and Chunk_ID for the dependent claim
                        # This is complex as igms only has SHI prefix and loc.
                        # For now, we'll assume if the SHI prefix is in final_verified_ssr, it's good enough
                        # A more robust solution would involve mapping claim_id to full SHI and Chunk_ID
                        dep_shi_prefix = next((igm['shi'] for igm in igms if igm['claim_id'] == dep_claim_id), None)
                        if dep_shi_prefix:
                            # Check if any SSR entry with this SHI prefix was verified
                            found_verified_dep = False
                            for ssr_key, ssr_val in final_verified_ssr.items():
                                if ssr_key.startswith(dep_shi_prefix) and ssr_val.get("Verification_Status") == "VERIFIED":
                                    found_verified_dep = True
                                    break
                            if not found_verified_dep:
                                all_deps_verified = False
                        else:
                            all_deps_verified = False
                            
                        if not all_deps_verified: # Break early if any dependency is not verified
                            break
                    
                    if all_deps_verified:
                        logic_model_name = var_entry.get('LOGIC_MODEL', 'Default_Verification_Model_v1.0')
                        var_entry["AUDIT_STATUS"] = "VERIFIED_LOGIC" 
                        log.info(f"Logic Model '{logic_model_name}' invoked for RM {rm['relationship_id']}. Status: {var_entry['AUDIT_STATUS']}")
                    else:
                        var_entry["AUDIT_STATUS"] = "INSUFFICIENT_PREMISE"
                    
                    final_verified_var[var_entry['RELATION_ID']] = var_entry
                else:
                    log.error(f"❌ Relationship {rm['relationship_id']} from candidate {i+1} failed lookup (RM not found in LLM's VAR).")

        # 4. Final Synthesis from Verified Claims
        # Populate verified_claims_texts AFTER all candidates have been processed
        for ssr_key, shi_entry in final_verified_ssr.items():
            if shi_entry.get("Verification_Status") == "VERIFIED":
                chunk_content = db.retrieve_chunk(shi_entry['SHI'], shi_entry['Chunk_ID'])
                if chunk_content and chunk_content['content'] not in verified_claims_texts:
                    verified_claims_texts.append(chunk_content['content'])

        if not verified_claims_texts:
            log.error("Agent generated content, but no claims could be successfully verified against the sources.")
            # Return an empty list for verified_claims_texts if no claims are verified
            return "❌ Agent generated content, but no claims could be successfully verified against the sources.", []

        # Combine all verified claims into a single context for final synthesis
        combined_verified_context = "\n\n".join(verified_claims_texts)

        synthesis_prompt = (
            f"Synthesize a concise, natural language answer to the query: '{query}' based on the following verified information. "
            f"Do NOT include any grounding markers (e.g., [C1:...], (R1:...)) or any other special formatting in your final answer.\n\n"
            f"Verified Information:\n---\n{combined_verified_context}\n---\n\n"
            f"Provide the synthesized answer now."
        )

        try:
            genai.configure(api_key=Config.GOOGLE_API_KEY)
            synthesis_model = genai.GenerativeModel("gemini-2.5-flash") 
            
            response = await synthesis_model.generate_content_async(synthesis_prompt)
            final_synthesized_answer = response.text
            
            log.debug(f"Final synthesized answer before ACG assembly: {final_synthesized_answer}")
            
        except Exception as e:
            log.error(f"Error during direct synthesis with google.generativeai: {e}", exc_info=True)
            final_synthesized_answer = "Error: Could not generate final synthesized answer."
        
        # Extract claim IDs from the verified_claims_texts for the final synthesis VAR entry
        all_verified_claim_ids = []
        for text in verified_claims_texts:
            igms_from_text, _, _, _ = self.ugvp.parse_acg_data(text)
            if igms_from_text:
                all_verified_claim_ids.extend([igm['claim_id'] for igm in igms_from_text])
        all_verified_claim_ids = list(set(all_verified_claim_ids)) # Remove duplicates

        final_synthesis_rel_type = "SUMMARY" if len(all_verified_claim_ids) > 1 else "INFERENCE"

        final_synthesis_relationship_metadata = {
            "RELATION_ID": f"R_FINAL_SYNTHESIS_{uuid.uuid4().hex[:8]}",
            "TYPE": final_synthesis_rel_type,
            "DEP_CLAIMS": all_verified_claim_ids,
            "SYNTHESIS_PROSE": f"Final synthesized answer to query: '{query}'",
            "LOGIC_MODEL": "ACG_Final_Synthesizer_Model_v1.0",
            "AUDIT_STATUS": "PENDING",
            "TIMESTAMP": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        }

        # Add this final synthesis VAR entry to the overall final_verified_var
        final_verified_var[final_synthesis_relationship_metadata['RELATION_ID']] = {
            **final_synthesis_relationship_metadata,
            "AUDIT_STATUS": "VERIFIED_LOGIC"
        }

        self._pre_response_hook()

        # The final_synthesized_answer itself doesn't contain ACG markers,
        # so we pass it directly and the assemble_final_output will add the ACG block.
        final_document = self.ugvp.assemble_final_output(final_synthesized_answer, final_verified_ssr, final_verified_var)

        log.info(f"Final GroundingAgent response: {final_document}")
        return final_document, verified_claims_texts # Return both the final document and the verified contexts

if __name__ == "__main__":
    
    async def main():
        parser = argparse.ArgumentParser(description="Run the UGVP Grounding Agent.")
        parser.add_argument(
            '--strategy', 
            type=str, 
            default='agent', 
            choices=['agent', 'llm'],
            help="The generation strategy to use: 'agent' (default) or 'llm'."
        )
        args = parser.parse_args()

        try:
            ugvp_instance = UGVPProtocol()
            agent = GroundingAgent(ugvp_instance, strategy=args.strategy)

            log.info(f"\n--- ACG Grounding Agent (PoC) Activated [Strategy: {args.strategy}] ---")
            log.info("This agent connects to MongoDB and verifies claims against live web pages.")
            
            rag_doc_count = db.count_rag_documents()
            if rag_doc_count == 0:
                log.warning("⚠️ RAG collection is empty. Please run the indexer to populate the database.")
                log.warning("Example: python ugvp_protocol/indexer.py <URL_TO_INDEX>")
            else:
                log.info(f"✅ RAG collection contains {rag_doc_count} documents.")

            log.info("Enter your query (e.g., 'What is the latest climate data?'). Type 'exit' to quit.")
            
            try:
                while True:
                    user_input = input("\n[USER]: ")
                    
                    if user_input.lower() in ['exit', 'q']:
                        break
                        
                    if not user_input.strip():
                        continue
                    
                    try:
                        response = await agent.generate_and_verify(user_input)
                        log.info(f"[AGENT RESPONSE]: {response}")
                    except Exception as e:
                        log.error(f"An error occurred during generation/verification: {e}", exc_info=True)
            finally:
                # Ensure agent and database resources are closed on exit
                if db:
                    db.close() 
                try:
                    import litellm
                    if hasattr(litellm, 'finish') and callable(litellm.finish):
                        await litellm.finish()
                except ImportError:
                    pass
                except Exception as e:
                    log.error(f"Error calling litellm.finish() during agent cleanup: {e}", exc_info=True)

        except Exception as e:
            log.critical(f"Agent failed to start/run due to critical error: {e}", exc_info=True)
            sys.exit(1)

    asyncio.run(main())
